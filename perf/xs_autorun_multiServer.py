#! /usr/bin/env python3

import argparse
import json
import os
import random
import shutil
import time
import sys
import getpass
import socket
from multiprocessing import Process, Queue
from gcpt import GCPT
import perf
import spec_score
from server import Server
from gcpt_run_time_eval import *
import copy
from tqdm import tqdm

""" PRAMETERS THAT NEED YOU CHECK """
tasks_dir = "SPEC06_EmuTasks_10_22_2021"
perf_base_path = ""
gcc12Enable = True
simFrontendTraceDir = "/nfs/home/wangzhizun/anzo/xs-env/NEMU/trace/2025-09-10_13-04-38"
emuArgR = "/nfs/home/share/liyanqin/old-gcpt-restorer/gcpt.bin"

ref_run_time_path = "/nfs/home/share/liyanqin/env-scripts/perf/json/gcc12o3-incFpcOff-jeMalloc-time.json"

def get_perf_base_path(xs_path):
  if os.path.isabs(tasks_dir):
    return tasks_dir
  return os.path.join(xs_path, tasks_dir)

def load_all_gcpt(gcpt_path, json_path, server_num, threads, state_filter=None, xs_path=None, sorted_by=None, report=False, dump_json_path=None, trace_dir="", benchmarks=""):
  perf_filter = [
    ("l3cache_mpki_load",      lambda x: float(x) < 3),
    ("branch_prediction_mpki", lambda x: float(x) > 5),
  ]
  perf_filter = None
  all_gcpt = []

  with open(json_path) as f:
    data = json.load(f)
  if benchmarks != "":
    benchmark_filter = set(benchmarks.replace(" ", "").split(","))
    data = {k:v for k,v in data.items() if k in benchmark_filter}
  with open(ref_run_time_path) as f:
    time_data = json.load(f)
  no_ref_run_time = False
  is_dump = dump_json_path is not None
  dump_json = {}
  error_json = {}
  if is_dump:
    dump_json = copy.deepcopy(data)
  if report:
    error_json = copy.deepcopy(data)
  perf_base_path = get_perf_base_path(xs_path)
  for benchspec in data:
    #if "gcc" not in benchspec:# or "hmmer" in benchspec:
    #  continue
    data_iterator = data[benchspec]["points"] if gcc12Enable else data[benchspec]
    for point in data_iterator:
      weight = data_iterator[point]
      hour = -1
      #FIXME: In order not to change the run json and get the time, here it's global time json temporaryly
      #the global time json is generated by `gcpt_run_time_json.py`
      if not no_ref_run_time and time_data.get(benchspec) and time_data[benchspec].get("times") and time_data[benchspec]["times"].get(point):
        hour = int(time_data[benchspec]["times"][point])
      elif not no_ref_run_time:
        no_ref_run_time = True
        print("The checkpoints of this json has NO REF RUN TIME.")
      gcpt = GCPT(gcpt_path, perf_base_path, benchspec, point, weight, hour, gcc12Enable, trace_dir)
      if state_filter is None and perf_filter is None:
        all_gcpt.append(gcpt)
        continue
      perf_match, state_match = True, True
      if state_filter is not None:
        state_match = False
        if gcpt.get_state() in state_filter:
          state_match = True
      if state_match and perf_filter is not None:
        perf_path = gcpt.get_err_path()
        counters = perf.PerfCounters(perf_path)
        counters.add_manip(get_all_manip())
        for fit in perf_filter:
          if not fit[1](counters[fit[0]]):
            perf_match = False
      if perf_match and state_match:
        all_gcpt.append(gcpt)
      elif is_dump:
        if gcc12Enable:
          del dump_json[benchspec]["points"][point]
          if len(dump_json[benchspec]["points"]) == 0:
            del dump_json[benchspec]
        else:
          del dump_json[benchspec][point]
          if len(dump_json[benchspec]) == 0:
            del dump_json[benchspec]
      # report error tests, which are not matched state_filter
      if state_match and report:
        if gcc12Enable:
          del error_json[benchspec]["points"][point]
          if len(error_json[benchspec]["points"]) == 0:
            del error_json[benchspec]
        else:
          del error_json[benchspec][point]
          if len(error_json[benchspec]) == 0:
            del error_json[benchspec]

  if not report:
    if sorted_by is not None:
      all_gcpt = sorted(all_gcpt, key=sorted_by)
    elif not no_ref_run_time:
      all_gcpt = sorted(all_gcpt, key=lambda x:-x.eval_run_time)
    else:
      #default sort, prioritize mcf
      all_gcpt = sorted(all_gcpt, key=lambda x:-1 if x.benchspec=="mcf" else -float(x.weight))

  if is_dump:
    with open(dump_json_path, "w") as f:
      json.dump(dump_json, f, indent=4)
  if report:
    print("=================== Unfinished / Aborted Tests ===================")
    json.dump(error_json, fp=sys.stdout, indent=4)
    print("\n")
  return all_gcpt

def get_server(server_list):
  l = []
  for s in server_list.strip().split(" "):
    l.append(Server(s))
  return l

def xs_run(server_list, workloads, xs_path, warmup, max_instr, threads, simFrontend = False, version=2006, dry_run=False, verbose=True, dump_db=False, emu_path=None):
  if emu_path is None:
    emu_path = os.path.join(xs_path, "build/emu")
  nemu_so_path = os.path.join(xs_path, "ready-to-run/riscv64-nemu-interpreter-so")
  # nemu_so_path = os.path.join(xs_path, "ready-to-run/riscv64-spike-so")
  base_arguments = [emu_path, '--diff', nemu_so_path, '--enable-fork', '-W', str(warmup), '-I', str(max_instr)]
  # base_arguments = [emu_path, '--diff', nemu_so_path, '-W', str(warmup), '-I', str(max_instr)]
  # base_arguments = [emu_path, '-W', str(warmup), '-I', str(max_instr)]
  if gcc12Enable and not simFrontend and not version == 2017:
    base_arguments = base_arguments + ['-r', emuArgR]
  if dump_db:
    base_arguments = base_arguments + ['--dump-db']
  base_arguments = base_arguments + ['-i']
  servers = get_server(server_list)
  def server_all_free():
    for s in servers:
      if not s.is_free():
        return False
    return True

  try:
    max_num = len(workloads)
    count = 0
    server_index_max = len(servers)
    server_index_flag = 0
    server_index_flag_next = 0
    for index in tqdm(range(max_num)):
      workload = workloads[index]
      random_seed = random.randint(0, 9999)
      run_cmd = base_arguments + [workload.get_bin_path()] + ["-s", f"{random_seed}"]
      if simFrontend:
        run_cmd = run_cmd + ['--instr-trace', workload.get_trace_path()]

      if not os.path.exists(workload.get_res_dir()):
        os.makedirs(workload.get_res_dir(), exist_ok=True)
      assigned = False
      while not assigned:
        # for sidx in random.sample(range(len(servers)), len(servers)):
        for sidx in range(server_index_flag, server_index_max):
          if dry_run and verbose:
            # dry_run will impact run_cmd print, so output it here
            print(run_cmd)
          if servers[sidx].assign(f"{workload}", run_cmd, threads, xs_path, workload.get_out_path(), workload.get_err_path(), dry_run, verbose):
            assigned = True
            count = count + 1
            server_index_flag_next = sidx
            break
        if not assigned or server_index_flag_next == server_index_max - 1:
          server_index_flag = 0
        else:
          server_index_flag = server_index_flag_next
        if not assigned:
          time.sleep(1)
          for s in servers:
            s.check_running()
      for s in servers:
        s.check_running()

    if not server_all_free():
      print("Waiting for pending tests to finish")
    while not server_all_free():
      time.sleep(1)
  except (Exception, SystemExit, KeyboardInterrupt, AssertionError) as e:
    print(f"An error occurred: {e}")
    print("Exception / Interrupt. Exiting all programs ...")

    pending_tests = []
    success_tests= []
    for s in servers:
      s.stop()
      print(f"{s.ipname} stopped")
      pending_tests = pending_tests + s.pending_tests()
      success_tests = success_tests + s.success_tests
    print(f"Finished {len(success_tests)}/{max_num}")
    print(f"Not started {max_num - count}/{max_num}:")
    if (count < max_num):
      for i in range(count, max_num):
        print(f"  ({i + 1 - count}) {workloads[i]}")
    print(f"Not finished {len(pending_tests)}/{max_num}:")
    for i, test in enumerate(pending_tests):
      print(f"  ({i+1}) {test}")

  failed_tests = []
  for s in servers:
    s.check_running()
    # s.stop()
    # print(f"{s.ipname} stopped")
    failed_tests = failed_tests + s.failed_tests
  if len(failed_tests) > 0:
    print(f"Errors {len(failed_tests)}/{max_num}:")
    for i, test in enumerate(failed_tests):
      print(f"  ({i + 1}) {test}")


def get_all_manip():
  all_manip = []
  ipc = perf.PerfManip(
    name = "IPC",
    counters = [f"clock_cycle", f"commitInstr"],
    func = lambda cycle, instr: instr * 1.0 / cycle
  )
  all_manip.append(ipc)
  l3cache_mpki_load = perf.PerfManip(
    name = "global.l3cache_mpki_load",
    counters = [
      "L3_bank_0_A_channel_AcquireBlock_fire", "L3_bank_0_A_channel_Get_fire",
      "L3_bank_1_A_channel_AcquireBlock_fire", "L3_bank_1_A_channel_Get_fire",
      "L3_bank_2_A_channel_AcquireBlock_fire", "L3_bank_2_A_channel_Get_fire",
      "L3_bank_3_A_channel_AcquireBlock_fire", "L3_bank_3_A_channel_Get_fire",
      "commitInstr"
    ],
    func = lambda fire1, fire2, fire3, fire4, fire5, fire6, fire7, fire8, instr :
    1000 * (fire1 + fire2 + fire3 + fire4 + fire5 + fire6 + fire7 + fire8) / instr
  )
  all_manip.append(l3cache_mpki_load)
  branch_mpki = perf.PerfManip(
    name = "global.branch_prediction_mpki",
    counters = ["ftq.BpWrong", "commitInstr"],
    func = lambda wrong, instr: 1000 * wrong / instr
  )
  all_manip.append(branch_mpki)
  return all_manip

def get_total_inst(benchspec, spec_version, isa):
  base_dir = "/nfs-nvme/home/share/checkpoints_profiles"
  if spec_version == 2006:
    if isa == "rv64gc_old":
      base_path = os.path.join(base_dir, "spec06_rv64gc_o2_50m/profiling")
      filename = "nemu_out.txt"
      bench_path = os.path.join(base_path, benchspec, filename)
    elif isa == "rv64gc":
      base_path = os.path.join(base_dir, "spec06_rv64gc_o2_20m/logs/profiling/")
      filename = benchspec + ".log"
      bench_path = os.path.join(base_path, filename)
    elif isa == "rv64gcb":
      base_path = os.path.join(base_dir, "spec06_rv64gcb_o2_20m/logs/profiling/")
      filename = benchspec + ".log"
      bench_path = os.path.join(base_path, filename)
    elif isa == "rv64gcb_o3":
      base_path = os.path.join(base_dir, "spec06_rv64gcb_o3_20m/logs/profiling/")
      filename = benchspec + ".log"
      bench_path = os.path.join(base_path, filename)
    else:
      print("Unknown ISA\n")
      return None
  elif spec_version == 2017:
    if isa == "rv64gc_old":
      base_path = os.path.join(base_dir, "spec17_rv64gc_o2_50m/profiling")
      filename = "nemu_out.txt"
      bench_path = os.path.join(base_path, benchspec, filename)
    elif isa == "rv64gcb":
      base_path = os.path.join(base_dir, "spec17_rv64gcb_o2_20m/logs/profiling/")
      filename = benchspec + ".log"
      bench_path = os.path.join(base_path, filename)
    elif isa == "rv64gcb_o3":
      base_path = os.path.join(base_dir, "spec17_rv64gcb_o3_20m/logs/profiling/")
      filename = benchspec + ".log"
      bench_path = os.path.join(base_path, filename)
    else:
      print("Unknown ISA\n")
      return None
  else:
    print("Unknown SPEC version\n")
    return None
  f = open(bench_path)
  for line in f:
    if "total guest instructions" in line:
      f.close()
      return int(line.split("instructions = ")[1].replace("\x1b[0m", ""))
  f.close()
  return None

def xs_report_ipc(xs_path, gcpt_queue, result_queue):
  while not gcpt_queue.empty():
    gcpt = gcpt_queue.get()
    # print(f"Processing {str(gcpt)}...")
    perf_path = gcpt.get_err_path()
    counters = perf.PerfCounters(perf_path)
    counters.add_manip(get_all_manip())
    # when the spec has not finished, IPC may be None
    if counters["IPC"] is not None:
      result_queue.put([gcpt.benchspec, [float(gcpt.weight), float(counters["IPC"])]])
    else:
      print("IPC not found in", gcpt.benchspec, gcpt.point, gcpt.weight)

def xs_report(all_gcpt, xs_path, spec_version, isa, num_jobs, json_path = None):
  # frequency/GHz
  frequency = 3
  if gcc12Enable:
    with open(json_path) as f:
      json_data = json.load(f)
  gcpt_ipc = dict()
  keys = list(map(lambda gcpt: gcpt.benchspec, all_gcpt))
  for k in keys:
    gcpt_ipc[k] = []
  # multi-threading for processing the performance counters
  gcpt_queue = Queue()
  for gcpt in all_gcpt:
    gcpt_queue.put(gcpt)
  result_queue = Queue()
  process_list = []
  for _ in range(num_jobs):
    p = Process(target=xs_report_ipc, args=(xs_path, gcpt_queue, result_queue))
    process_list.append(p)
    p.start()
  for p in process_list:
    p.join()
  while not result_queue.empty():
    result = result_queue.get()
    gcpt_ipc[result[0]].append(result[1])
  # print("=================== Coverage ==================")
  spec_time = {}
  spec_weight = {}
  spec_weight_list = {}
  spec_instr_list = {}
  for benchspec in gcpt_ipc:
    total_weight = sum(map(lambda info: info[0], gcpt_ipc[benchspec]))
    total_cpi = sum(map(lambda info: info[0] / info[1], gcpt_ipc[benchspec])) / total_weight
    if gcc12Enable:
      num_instr = int(json_data[benchspec]["insts"])
    else:
      num_instr = get_total_inst(benchspec, spec_version, isa)
    num_seconds = total_cpi * num_instr / (frequency * (10 ** 9))
    # print(f"{benchspec:>25} coverage: {total_weight:.2f}")
    spec_name = benchspec.split("_")[0]
    spec_time[spec_name] = spec_time.get(spec_name, 0) + num_seconds
    spec_weight_list[spec_name] = spec_weight_list.get(spec_name, []) + [total_weight]
    spec_instr_list[spec_name] = spec_instr_list.get(spec_name, []) + [num_instr]
  for key in spec_weight_list:
    spec_weight[key] = np.sum(np.multiply(spec_weight_list[key], spec_instr_list[key]))/np.sum(spec_instr_list[key])
  # print()
  spec_score.get_spec_score(spec_time, spec_version, frequency, spec_weight)
  print(f"Number of Checkpoints: {len(all_gcpt)}")
  print(f"SPEC CPU Version: SPEC CPU{spec_version}, {isa}")
  # check dramsim3
  with open(all_gcpt[0].get_out_path()) as f:
    flag = True
    for line in f:
      if "DRAMSIM3 config" in line:
        print("DRAMSIM3 config:", line.split("DRAMSIM3 config:")[1].strip())
        flag = False
        break
    if flag:
      print("[WARNING] No DRAMSIM3 config found! Please check whether DRAMSIM3 is enabled correctly.")
      return

def xs_show(all_gcpt):
  for gcpt in all_gcpt:
    gcpt.show()

def xs_debug(all_gcpt):
  for gcpt in all_gcpt:
    gcpt.debug()

if __name__ == "__main__":
  # --show for already running result, including "name, state, ipc, sim speed"
  # --debug for error tests
  # --report for spec scores
  parser = argparse.ArgumentParser(description="autorun script for xs")
  parser.add_argument('gcpt_path', metavar='gcpt_path', type=str,
                      help='path to gcpt checkpoints')
  parser.add_argument('json_path', metavar='json_path', type=str,
                      help='path to gcpt json')
  parser.add_argument('--xs', help='path to xs')
  parser.add_argument('--xs-emu', default=None, type=str, help='path to xs emulator elf (overrides default xs_path/build/emu)')
  parser.add_argument('--ref', default=None, type=str, help='path to ref')
  parser.add_argument('--warmup', '-W', default=20000000, type=int, help="warmup instr count")
  parser.add_argument('--max-instr', '-I', default=40000000, type=int, help="max instr count")
  parser.add_argument('--threads', '-T', default=1, type=int, help="number of emu threads")
  parser.add_argument('--server-list', '-L', type=str, help="server list, like \"172.28.9.104 172.28.9.107\", support alias")
  parser.add_argument('--report', '-R', action='store_true', default=False, help='report only')
  parser.add_argument('--show', '-S', action='store_true', default=False, help='show list of gcpt only')
  parser.add_argument('--debug', '-D', action='store_true', default=False, help='debug options')
  parser.add_argument('--check', '-C', action='store_true', default=False, help='debug options')
  parser.add_argument('--sim-frontend', '-F', action='store_true', default=False, help='use trace for sim frontend')
  parser.add_argument('--dump-json-path', type=str, help='dump the json path of filter gcpt')
  parser.add_argument('--version', default=2006, type=int, help='SPEC version')
  parser.add_argument('--isa', default="rv64gcb", type=str, help='ISA version')
  parser.add_argument('--dir', default=None, type=str, help='SPECTasks dir')
  parser.add_argument('--jobs', '-j', default=32, type=int, help="processing files in 'j' threads")
  parser.add_argument('--resume', action='store_true', default=False, help="continue to exe, ignore the aborted and success tests")
  parser.add_argument('--dry-run', action='store_true', default=False, help="does not run real simulation")
  parser.add_argument('--verbose', '-v', action='store_true', default=False, help="display more outputs")
  parser.add_argument('--dump-db', action='store_true', default=False, help="dump database for necessary tests")
  parser.add_argument('--benchmarks', default='', type=str, help='Specific benchmarks to run (comma-separated), leave empty to run all')

  args = parser.parse_args()

  if args.dir is not None:
    tasks_dir = args.dir
  perf_base_path = get_perf_base_path(args.xs)

  if args.xs is None:
    print("need --xs")
    sys.exit()

  if args.ref is None:
    args.ref = args.xs

  if args.server_list is None:
    args.server_list = socket.gethostname()
    server_num = 1
  else:
    server_num = len(args.server_list.strip().split(" "))

  if args.show:
    gcpt = load_all_gcpt(args.gcpt_path, args.json_path, server_num, args.threads, xs_path=args.xs, benchmarks=args.benchmarks)
    #gcpt = load_all_gcpt(args.gcpt_path, args.json_path,
      #state_filter=[GCPT.STATE_FINISHED], xs_path=args.ref, sorted_by=lambda x: x.get_simulation_cps())
      #state_filter=[GCPT.STATE_ABORTED], xs_path=args.ref, sorted_by=lambda x: x.get_ipc())
      #state_filter=[GCPT.STATE_ABORTED], xs_path=args.ref, sorted_by=lambda x: x.benchspec.lower())
      #state_filter=[GCPT.STATE_RUNNING], xs_path=args.ref, sorted_by=lambda x: x.benchspec.lower())
      #state_filter=[GCPT.STATE_FINISHED], xs_path=args.ref, sorted_by=lambda x: -x.num_cycles)
      #state_filter=[GCPT.STATE_ABORTED], xs_path=args.ref, sorted_by=lambda x: -x.num_cycles)
    xs_show(gcpt)
  elif args.check:
    gcpt = load_all_gcpt(
      args.gcpt_path, args.json_path, server_num, args.threads,
      # state_filter=[GCPT.STATE_FINISHED], # what you wanna check
      state_filter=[GCPT.STATE_NONE, GCPT.STATE_ABORTED, GCPT.STATE_RUNNING],
      xs_path=args.ref,
      dump_json_path=args.dump_json_path, # dump checked json
      benchmarks=args.benchmarks
    )
  elif args.debug:
    gcpt = load_all_gcpt(args.gcpt_path, args.json_path, server_num, args.threads,
                         state_filter=[GCPT.STATE_ABORTED],
                         xs_path=args.xs,
                         sorted_by=lambda x: -x.num_cycles,
                         benchmarks=args.benchmarks
                         )
    xs_debug(gcpt)
  elif args.report:
    gcpt = load_all_gcpt(args.gcpt_path, args.json_path, server_num, args.threads,
                         state_filter=[GCPT.STATE_FINISHED], xs_path=args.xs, sorted_by=lambda x: x.benchspec.lower(), report=True, benchmarks=args.benchmarks)
    xs_report(gcpt, args.ref, args.version, args.isa, args.jobs, args.json_path)
  else:
    state_filter = None
    print("RESUME:", args.resume)
    if args.resume:
      state_filter = [GCPT.STATE_RUNNING, GCPT.STATE_NONE]
    # If just wanna run aborted test, change the script.
    gcpt = load_all_gcpt(
      args.gcpt_path, args.json_path, server_num, args.threads,
      state_filter=state_filter,
      xs_path=args.xs,
      trace_dir=simFrontendTraceDir if args.sim_frontend else "",
      benchmarks=args.benchmarks
      #both time and coverage are taken into account, but to be evaluated
      # sorted_by=lambda x: -(x.eval_run_time * float(x.weight))
    )
    #gcpt = load_all_gcpt(args.gcpt_path, args.json_path)
    #gcpt = load_all_gcpt(args.gcpt_path, args.json_path,
      #state_filter=[GCPT.STATE_ABORTED], xs_path=args.ref, sorted_by=lambda x: -x.num_cycles)
      #state_filter=[GCPT.STATE_FINISHED], xs_path=args.ref, sorted_by=lambda x: -x.num_cycles)
      #state_filter=[GCPT.STATE_ABORTED], xs_path=args.ref, sorted_by=lambda x: x.benchspec.lower())
      #state_filter=[GCPT.STATE_ABORTED], xs_path=args.ref, sorted_by=lambda x: x.get_ipc())
      #state_filter=[GCPT.STATE_RUNNING], xs_path=args.ref, sorted_by=lambda x: x.benchspec.lower())
    if (len(gcpt) == 0):
      print("All the tests are already finished.")
      print(f"perf_base_path: {perf_base_path}")
      sys.exit()
    print("All:  ", len(gcpt))
    print("First:", gcpt[0])
    print("Last: ", gcpt[-1])
    xs_run(args.server_list, gcpt, args.xs, args.warmup, args.max_instr, args.threads, args.sim_frontend, args.version, args.dry_run, args.verbose, args.dump_db, args.xs_emu)
